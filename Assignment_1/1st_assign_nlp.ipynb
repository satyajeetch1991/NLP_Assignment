{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Satyajeet Tukaram Chavan\n",
        "### Roll no-381025\n",
        "### PRN no-22310630\n",
        "### Batch-A1"
      ],
      "metadata": {
        "id": "vCB41DtaA55q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform tokenization (Whitespace, Punctuation-based, Treebank, Tweet, MWE) using NLTK\n",
        "library. Use porter stemmer and snowball stemmer for stemming. Use any technique for\n",
        "lemmatization."
      ],
      "metadata": {
        "id": "0WTJEddCA14J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcnJSVRfRMcM"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, TweetTokenizer, TreebankWordTokenizer, MWETokenizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2iFda5RRx-5",
        "outputId": "7c0793bd-8c59-411d-9a99-22f6990f529f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NLTK is a powerful library! It's used for Natural Language Processing tasks.\\nLet's test tokenization, stemming, and lemmatization. #NLP @student\""
      ],
      "metadata": {
        "id": "p6w4hEA_R0yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Text:\\n\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TR1vCHMUR3B9",
        "outputId": "fb989c36-5cab-46bd-cfd6-e478ff4d0da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " NLTK is a powerful library! It's used for Natural Language Processing tasks.\n",
            "Let's test tokenization, stemming, and lemmatization. #NLP @student\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "whitespace_tokens = text.split()\n",
        "print(\"\\nWhitespace Tokenization:\\n\", whitespace_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncf28r9RR5dK",
        "outputId": "8c60014c-e315-4611-ed68-b68012911b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Whitespace Tokenization:\n",
            " ['NLTK', 'is', 'a', 'powerful', 'library!', \"It's\", 'used', 'for', 'Natural', 'Language', 'Processing', 'tasks.', \"Let's\", 'test', 'tokenization,', 'stemming,', 'and', 'lemmatization.', '#NLP', '@student']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punct_tokens = nltk.tokenize.wordpunct_tokenize(text)\n",
        "print(\"\\nPunctuation-based Tokenization:\\n\", punct_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJQD0u9rR9hK",
        "outputId": "a7009bbc-ff64-4511-98ef-7c6f250afe3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Punctuation-based Tokenization:\n",
            " ['NLTK', 'is', 'a', 'powerful', 'library', '!', 'It', \"'\", 's', 'used', 'for', 'Natural', 'Language', 'Processing', 'tasks', '.', 'Let', \"'\", 's', 'test', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.', '#', 'NLP', '@', 'student']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Treebank Tokenization\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "treebank_tokens = treebank_tokenizer.tokenize(text)\n",
        "print(\"\\nTreebank Tokenization:\\n\", treebank_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oltW7hiyR_0U",
        "outputId": "31986778-f773-454f-b733-6745fcb7de56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treebank Tokenization:\n",
            " ['NLTK', 'is', 'a', 'powerful', 'library', '!', 'It', \"'s\", 'used', 'for', 'Natural', 'Language', 'Processing', 'tasks.', 'Let', \"'s\", 'test', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization.', '#', 'NLP', '@', 'student']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Tweet Tokenization\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
        "print(\"\\nTweet Tokenization:\\n\", tweet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2Ci6MgeSU6n",
        "outputId": "0597cc43-f474-4d73-b858-508929907aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tweet Tokenization:\n",
            " ['NLTK', 'is', 'a', 'powerful', 'library', '!', \"It's\", 'used', 'for', 'Natural', 'Language', 'Processing', 'tasks', '.', \"Let's\", 'test', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.', '#NLP', '@student']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Multi-Word Expression (MWE) Tokenization\n",
        "mwe = MWETokenizer([('Natural', 'Language', 'Processing'), ('New', 'York')], separator='_')\n",
        "mwe_tokens = mwe.tokenize(word_tokenize(text))\n",
        "print(\"\\nMWE Tokenization:\\n\", mwe_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gzyzgv37SYkO",
        "outputId": "1ca17799-d1d7-4e77-8c26-cf213e6e9e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MWE Tokenization:\n",
            " ['NLTK', 'is', 'a', 'powerful', 'library', '!', 'It', \"'s\", 'used', 'for', 'Natural_Language_Processing', 'tasks', '.', 'Let', \"'s\", 'test', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.', '#', 'NLP', '@', 'student']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "porter = PorterStemmer()\n",
        "snowball = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "O-EXN379SbSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter_stems = [porter.stem(word) for word in treebank_tokens]\n",
        "snowball_stems = [snowball.stem(word) for word in treebank_tokens]"
      ],
      "metadata": {
        "id": "uFxDan8fS-h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPorter Stemmer Output:\\n\", porter_stems)\n",
        "print(\"\\nSnowball Stemmer Output:\\n\", snowball_stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tN6uDLfTAff",
        "outputId": "4fbdc5ac-ab1b-407b-b88d-e41a97d3427e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Porter Stemmer Output:\n",
            " ['nltk', 'is', 'a', 'power', 'librari', '!', 'it', \"'s\", 'use', 'for', 'natur', 'languag', 'process', 'tasks.', 'let', \"'s\", 'test', 'token', ',', 'stem', ',', 'and', 'lemmatization.', '#', 'nlp', '@', 'student']\n",
            "\n",
            "Snowball Stemmer Output:\n",
            " ['nltk', 'is', 'a', 'power', 'librari', '!', 'it', \"'s\", 'use', 'for', 'natur', 'languag', 'process', 'tasks.', 'let', \"'s\", 'test', 'token', ',', 'stem', ',', 'and', 'lemmatization.', '#', 'nlp', '@', 'student']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in treebank_tokens]"
      ],
      "metadata": {
        "id": "pmC72aXBTCOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLemmatization Output:\\n\", lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEEkxW3cTFcl",
        "outputId": "44b9aba7-a0c5-4971-c2ef-2c92b159c3d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatization Output:\n",
            " ['NLTK', 'is', 'a', 'powerful', 'library', '!', 'It', \"'s\", 'used', 'for', 'Natural', 'Language', 'Processing', 'tasks.', 'Let', \"'s\", 'test', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization.', '#', 'NLP', '@', 'student']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5NGbWQh2THu1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}