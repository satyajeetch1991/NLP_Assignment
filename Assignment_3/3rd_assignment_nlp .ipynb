{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Satyajeet Tukaram Chavan\n",
        "### Roll no-381025\n",
        "### PRN no-22310630\n",
        "### Batch-A1"
      ],
      "metadata": {
        "id": "Ff7PBFCywtPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform text cleaning, perform lemmatization (any method), remove stop words (any method),\n",
        "label encoding. Create representations using TF-IDF. Save outputs"
      ],
      "metadata": {
        "id": "zZEABAzcm3AZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tYhZJ3llwt7"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW44e9irnwpb",
        "outputId": "71603a34-c5b1-42ee-fc49-ee55b91b23eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Dataset\n",
        "# -------------------------------\n",
        "data = {\n",
        "\"text\": [\n",
        "\"Natural Language Processing is amazing!\",\n",
        "\"Machine learning helps computers understand language.\",\n",
        "\"TF-IDF and NLP are important techniques.\",\n",
        "\"Stop words should be removed from text\",\n",
        "\"Lemmatization converts words into base form\"\n",
        "],\n",
        "\"label\": [\"tech\", \"tech\", \"tech\", \"preprocess\", \"preprocess\"]\n",
        "}"
      ],
      "metadata": {
        "id": "_0i_nkrjn3Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Original Dataset:\\n\", df)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Text Cleaning\n",
        "# -------------------------------\n",
        "def clean_text(text):\n",
        " text = text.lower() # Convert to lowercase\n",
        " text = re.sub(r'[^a-z\\s]', '', text) # Remove numbers and punctuation\n",
        " text = re.sub(r'\\s+', ' ', text).strip() # Remove extra spaces\n",
        " return text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKeaw2ZKn7bL",
        "outputId": "0fb2c1fe-9ef3-43a3-cae4-e8499f33f332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "                                                 text       label\n",
            "0            Natural Language Processing is amazing!        tech\n",
            "1  Machine learning helps computers understand la...        tech\n",
            "2           TF-IDF and NLP are important techniques.        tech\n",
            "3             Stop words should be removed from text  preprocess\n",
            "4        Lemmatization converts words into base form  preprocess\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply cleaning\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "print(\"\\nAfter Text Cleaning:\\n\", df['clean_text'])\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Stopword Removal\n",
        "# -------------------------------\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "def remove_stopwords(text):\n",
        " words = nltk.word_tokenize(text)\n",
        " filtered_words = [word for word in words if word not in stop_words]\n",
        " return ' '.join(filtered_words)\n",
        "\n",
        "\n",
        "# Apply stopword removal\n",
        "df['no_stopwords'] = df['clean_text'].apply(remove_stopwords)\n",
        "print(\"\\nAfter Stopword Removal:\\n\", df['no_stopwords'])\n",
        "\n",
        "\n",
        "# --------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxQSR8y_oBlV",
        "outputId": "2a434ff9-8fdb-4360-e513-98b652ed9c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After Text Cleaning:\n",
            " 0               natural language processing is amazing\n",
            "1    machine learning helps computers understand la...\n",
            "2               tfidf and nlp are important techniques\n",
            "3               stop words should be removed from text\n",
            "4          lemmatization converts words into base form\n",
            "Name: clean_text, dtype: object\n",
            "\n",
            "After Stopword Removal:\n",
            " 0                  natural language processing amazing\n",
            "1    machine learning helps computers understand la...\n",
            "2                       tfidf nlp important techniques\n",
            "3                              stop words removed text\n",
            "4               lemmatization converts words base form\n",
            "Name: no_stopwords, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Lemmatization\n",
        "# -------------------------------\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def lemmatize_text(text):\n",
        " words = nltk.word_tokenize(text)\n",
        " lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
        " return ' '.join(lemmas)\n",
        "\n",
        "\n",
        "# Apply lemmatization\n",
        "df['lemmatized_text'] = df['no_stopwords'].apply(lemmatize_text)\n",
        "print(\"\\nAfter Lemmatization:\\n\", df['lemmatized_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Alc7fpqioTs3",
        "outputId": "6aece7ce-b7d4-456e-da0b-6e405d99406f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After Lemmatization:\n",
            " 0                  natural language processing amazing\n",
            "1    machine learning help computer understand lang...\n",
            "2                        tfidf nlp important technique\n",
            "3                               stop word removed text\n",
            "4                 lemmatization convert word base form\n",
            "Name: lemmatized_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Label Encoding\n",
        "# -------------------------------\n",
        "label_encoder = LabelEncoder()\n",
        "df['encoded_label'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "\n",
        "print(\"\\nLabel Encoding:\")\n",
        "for label, encoded in zip(df['label'], df['encoded_label']):\n",
        "    print(f\"{label} -> {encoded}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQIAqWmbpxXx",
        "outputId": "21b7e439-028d-4eea-82af-407522e966cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Label Encoding:\n",
            "tech -> 1\n",
            "tech -> 1\n",
            "tech -> 1\n",
            "preprocess -> 0\n",
            "preprocess -> 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78504fd6",
        "outputId": "df3190d6-0625-4edc-c598-ac8a5249b37f"
      },
      "source": [
        "# 5. TF-IDF Representation\n",
        "# -------------------------------\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['lemmatized_text'])\n",
        "\n",
        "\n",
        "# Convert TF-IDF matrix to DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"\\nTF-IDF Matrix:\\n\", tfidf_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Matrix:\n",
            "     amazing      base  computer   convert      form      help  important  \\\n",
            "0  0.523358  0.000000  0.000000  0.000000  0.000000  0.000000        0.0   \n",
            "1  0.000000  0.000000  0.420669  0.000000  0.000000  0.420669        0.0   \n",
            "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000        0.5   \n",
            "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000        0.0   \n",
            "4  0.000000  0.463693  0.000000  0.463693  0.463693  0.000000        0.0   \n",
            "\n",
            "   language  learning  lemmatization  ...   natural  nlp  processing  \\\n",
            "0  0.422242  0.000000       0.000000  ...  0.523358  0.0    0.523358   \n",
            "1  0.339393  0.420669       0.000000  ...  0.000000  0.0    0.000000   \n",
            "2  0.000000  0.000000       0.000000  ...  0.000000  0.5    0.000000   \n",
            "3  0.000000  0.000000       0.000000  ...  0.000000  0.0    0.000000   \n",
            "4  0.000000  0.000000       0.463693  ...  0.000000  0.0    0.000000   \n",
            "\n",
            "    removed      stop  technique      text  tfidf  understand      word  \n",
            "0  0.000000  0.000000        0.0  0.000000    0.0    0.000000  0.000000  \n",
            "1  0.000000  0.000000        0.0  0.000000    0.0    0.420669  0.000000  \n",
            "2  0.000000  0.000000        0.5  0.000000    0.5    0.000000  0.000000  \n",
            "3  0.523358  0.523358        0.0  0.523358    0.0    0.000000  0.422242  \n",
            "4  0.000000  0.000000        0.0  0.000000    0.0    0.000000  0.374105  \n",
            "\n",
            "[5 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Save Outputs to Files\n",
        "# -------------------------------\n",
        "# Save cleaned and processed dataset\n",
        "df.to_csv(\"processed_text_data.csv\", index=False)\n",
        "\n",
        "\n",
        "# Save TF-IDF features\n",
        "tfidf_df.to_csv(\"tfidf_features.csv\", index=False)\n",
        "\n",
        "\n",
        "print(\"\\nFiles saved successfully:\")\n",
        "print(\"1. processed_text_data.csv\")\n",
        "print(\"2. tfidf_features.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOj-PPgds2c4",
        "outputId": "6a5e6e80-f46e-43b0-868a-4d7c4df0c419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Files saved successfully:\n",
            "1. processed_text_data.csv\n",
            "2. tfidf_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wWzPm1PpuCjQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}