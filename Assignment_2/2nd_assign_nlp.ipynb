{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Satyajeet Tukaram Chavan\n",
        "### Roll no-381025\n",
        "### PRN no-22310630\n",
        "### Batch-A1"
      ],
      "metadata": {
        "id": "JbFia5icB9rF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform bag-of-words approach (count occurrence, normalized count occurrence), TF-IDF on\n",
        "data. Create embeddings using Word2Vec"
      ],
      "metadata": {
        "id": "ry-Brls6CGpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk scikit-learn gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35VEuCXBUqmT",
        "outputId": "94c99859-6742-452a-db5e-ebb9b944db35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZvspNGUUPuO"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEdvXdJ4Ufio",
        "outputId": "75d1603e-f359-4907-c19b-a9a8cd52b22e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample dataset (you can replace this with your own data)\n",
        "corpus = [\n",
        "\"Natural Language Processing is a fascinating field\",\n",
        "\"NLTK and Gensim are useful libraries for NLP\",\n",
        "\"Machine learning helps computers understand language\",\n",
        "\"Word embeddings capture semantic meaning\",\n",
        "\"TF IDF and Bag of Words are basic text features\"\n",
        "]"
      ],
      "metadata": {
        "id": "uh9RSFOaUuX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Corpus:\")\n",
        "for i, doc in enumerate(corpus, 1):\n",
        " print(f\"{i}. {doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZlh1Xl-Uy_t",
        "outputId": "700425b5-76a8-43a6-f529-56ce47a29241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Corpus:\n",
            "1. Natural Language Processing is a fascinating field\n",
            "2. NLTK and Gensim are useful libraries for NLP\n",
            "3. Machine learning helps computers understand language\n",
            "4. Word embeddings capture semantic meaning\n",
            "5. TF IDF and Bag of Words are basic text features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Bag-of-Words (Count Occurrence)\n",
        "# -------------------------------\n",
        "count_vectorizer = CountVectorizer()\n",
        "bow_counts = count_vectorizer.fit_transform(corpus)\n",
        "\n",
        "\n",
        "print(\"\\nBag-of-Words (Count Occurrence):\")\n",
        "print(bow_counts.toarray())\n",
        "print(\"Feature Names:\")\n",
        "print(count_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_kIBvVkU2bO",
        "outputId": "b7c93b32-95a3-49ac-c9b6-723d99fa9d0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bag-of-Words (Count Occurrence):\n",
            "[[0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0]\n",
            " [1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1]]\n",
            "Feature Names:\n",
            "['and' 'are' 'bag' 'basic' 'capture' 'computers' 'embeddings'\n",
            " 'fascinating' 'features' 'field' 'for' 'gensim' 'helps' 'idf' 'is'\n",
            " 'language' 'learning' 'libraries' 'machine' 'meaning' 'natural' 'nlp'\n",
            " 'nltk' 'of' 'processing' 'semantic' 'text' 'tf' 'understand' 'useful'\n",
            " 'word' 'words']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Normalized Bag-of-Words\n",
        "# -------------------------------\n",
        "count_vectorizer_norm = CountVectorizer()\n",
        "bow_counts_for_norm = count_vectorizer_norm.fit_transform(corpus)\n",
        "\n",
        "tfidf_transformer = TfidfTransformer(use_idf=False, norm='l2')\n",
        "bow_normalized = tfidf_transformer.fit_transform(bow_counts_for_norm)\n",
        "\n",
        "print(\"\\nNormalized Bag-of-Words (L2 Norm):\")\n",
        "print(bow_normalized.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoegyHL2VBss",
        "outputId": "5c4d7652-919d-46ee-cfb3-ffb6df11449d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Normalized Bag-of-Words (L2 Norm):\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.40824829 0.         0.40824829 0.         0.\n",
            "  0.         0.         0.40824829 0.40824829 0.         0.\n",
            "  0.         0.         0.40824829 0.         0.         0.\n",
            "  0.40824829 0.         0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.35355339 0.35355339 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.35355339 0.35355339\n",
            "  0.         0.         0.         0.         0.         0.35355339\n",
            "  0.         0.         0.         0.35355339 0.35355339 0.\n",
            "  0.         0.         0.         0.         0.         0.35355339\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.40824829\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.40824829 0.         0.         0.40824829 0.40824829 0.\n",
            "  0.40824829 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.40824829 0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.4472136  0.\n",
            "  0.4472136  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.4472136  0.         0.         0.         0.\n",
            "  0.         0.4472136  0.         0.         0.         0.\n",
            "  0.4472136  0.        ]\n",
            " [0.31622777 0.31622777 0.31622777 0.31622777 0.         0.\n",
            "  0.         0.         0.31622777 0.         0.         0.\n",
            "  0.         0.31622777 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.31622777\n",
            "  0.         0.         0.31622777 0.31622777 0.         0.\n",
            "  0.         0.31622777]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zpd1j-UJVEeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4223d8fc",
        "outputId": "386c12a2-87f2-47f2-936a-86a323ef92cd"
      },
      "source": [
        "# 3. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "# --------------------------------------------------\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n",
        "print(\"Feature Names (TF-IDF):\")\n",
        "print(tfidf_vectorizer.get_feature_names_out())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.42066906 0.         0.42066906 0.         0.\n",
            "  0.         0.         0.42066906 0.33939315 0.         0.\n",
            "  0.         0.         0.42066906 0.         0.         0.\n",
            "  0.42066906 0.         0.         0.         0.         0.\n",
            "  0.         0.        ]\n",
            " [0.29857028 0.29857028 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.37007017 0.37007017\n",
            "  0.         0.         0.         0.         0.         0.37007017\n",
            "  0.         0.         0.         0.37007017 0.37007017 0.\n",
            "  0.         0.         0.         0.         0.         0.37007017\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.42066906\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.42066906 0.         0.         0.33939315 0.42066906 0.\n",
            "  0.42066906 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.42066906 0.\n",
            "  0.         0.        ]\n",
            " [0.         0.         0.         0.         0.4472136  0.\n",
            "  0.4472136  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.4472136  0.         0.         0.         0.\n",
            "  0.         0.4472136  0.         0.         0.         0.\n",
            "  0.4472136  0.        ]\n",
            " [0.26453202 0.26453202 0.32788062 0.32788062 0.         0.\n",
            "  0.         0.         0.32788062 0.         0.         0.\n",
            "  0.         0.32788062 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.32788062\n",
            "  0.         0.         0.32788062 0.32788062 0.         0.\n",
            "  0.         0.32788062]]\n",
            "Feature Names (TF-IDF):\n",
            "['and' 'are' 'bag' 'basic' 'capture' 'computers' 'embeddings'\n",
            " 'fascinating' 'features' 'field' 'for' 'gensim' 'helps' 'idf' 'is'\n",
            " 'language' 'learning' 'libraries' 'machine' 'meaning' 'natural' 'nlp'\n",
            " 'nltk' 'of' 'processing' 'semantic' 'text' 'tf' 'understand' 'useful'\n",
            " 'word' 'words']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d6b00f4"
      },
      "source": [
        "# 4. Word2Vec Embeddings\n",
        "# -------------------------------\n",
        "# Tokenize corpus\n",
        "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "511dc4b1",
        "outputId": "df5e2d81-8947-4540-b95c-8cc57ac0652e"
      },
      "source": [
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "print(\"\\nWord2Vec Model Trained.\")\n",
        "\n",
        "# Example: Get embedding for a word\n",
        "if 'language' in word2vec_model.wv:\n",
        "    print(\"\\nEmbedding for 'language':\")\n",
        "    print(word2vec_model.wv['language'])\n",
        "\n",
        "# Example: Find most similar words\n",
        "if 'language' in word2vec_model.wv:\n",
        "    print(\"\\nWords most similar to 'language':\")\n",
        "    print(word2vec_model.wv.most_similar('language'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word2Vec Model Trained.\n",
            "\n",
            "Embedding for 'language':\n",
            "[ 9.3217925e-05  3.0777205e-03 -6.8114577e-03 -1.3711961e-03\n",
            "  7.6678363e-03  7.3385513e-03 -3.6672817e-03  2.6534828e-03\n",
            " -8.3205597e-03  6.1965222e-03 -4.6336860e-03 -3.1653163e-03\n",
            "  9.3114022e-03  8.7925693e-04  7.4929553e-03 -6.0722707e-03\n",
            "  5.1641441e-03  9.9267988e-03 -8.4607024e-03 -5.1381774e-03\n",
            " -7.0598871e-03 -4.8623248e-03 -3.7750429e-03 -8.5290810e-03\n",
            "  7.9560699e-03 -4.8423950e-03  8.4230630e-03  5.2665914e-03\n",
            " -6.5490673e-03  3.9536874e-03  5.4720147e-03 -7.4247816e-03\n",
            " -7.4070757e-03 -2.4801653e-03 -8.6230347e-03 -1.5786547e-03\n",
            " -3.9272258e-04  3.2925415e-03  1.4375689e-03 -8.8077754e-04\n",
            " -5.5948677e-03  1.7290696e-03 -8.9702714e-04  6.7909225e-03\n",
            "  3.9776331e-03  4.5299693e-03  1.4313816e-03 -2.6992778e-03\n",
            " -4.3622991e-03 -1.0327762e-03  1.4375491e-03 -2.6472486e-03\n",
            " -7.0811724e-03 -7.8063812e-03 -9.1194492e-03 -5.9319953e-03\n",
            " -1.8469368e-03 -4.3250942e-03 -6.4575053e-03 -3.7153910e-03\n",
            "  4.2891768e-03 -3.7354406e-03  8.3837025e-03  1.5354720e-03\n",
            " -7.2417236e-03  9.4369892e-03  7.6282066e-03  5.4993257e-03\n",
            " -6.8531246e-03  5.8244858e-03  4.0129237e-03  5.1872390e-03\n",
            "  4.2567030e-03  1.9403519e-03 -3.1674094e-03  8.3520999e-03\n",
            "  9.6120248e-03  3.7981323e-03 -2.8402938e-03  4.6376235e-06\n",
            "  1.2150808e-03 -8.4561808e-03 -8.2211411e-03 -2.2700263e-04\n",
            "  1.2355124e-03 -5.7434598e-03 -4.7176727e-03 -7.3452839e-03\n",
            "  8.3272345e-03  1.1881869e-04 -4.5084413e-03  5.6984951e-03\n",
            "  9.1811111e-03 -4.0971716e-03  7.9733087e-03  5.3742304e-03\n",
            "  5.8812676e-03  5.0452800e-04  8.2134595e-03 -7.0171030e-03]\n",
            "\n",
            "Words most similar to 'language':\n",
            "[('bag', 0.1993369609117508), ('meaning', 0.17277774214744568), ('a', 0.1712634563446045), ('text', 0.1701083779335022), ('helps', 0.15302231907844543), ('machine', 0.1486770063638687), ('words', 0.14592388272285461), ('natural', 0.0806151032447815), ('basic', 0.06411021947860718), ('field', 0.058455903083086014)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6cc8062",
        "outputId": "1dc727b8-608b-4a0d-a1a0-4bad946586ba"
      },
      "source": [
        "# Download required NLTK data for word_tokenize\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get sentence embedding (average of word vectors)\n",
        "def sentence_embedding(sentence, model):\n",
        "    tokens = word_tokenize(sentence.lower())\n",
        "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if len(vectors) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "sample_sentence = \"Natural language processing\"\n",
        "embedding = sentence_embedding(sample_sentence, word2vec_model)\n",
        "\n",
        "print(f\"\\nSentence embedding for: '{sample_sentence}'\")\n",
        "print(embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA22oeiwVvP4",
        "outputId": "c139f674-3c1d-4a26-ecaf-dc44e59f9d0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence embedding for: 'Natural language processing'\n",
            "[ 1.43745320e-03 -5.19340346e-03 -1.62053725e-03 -3.62594059e-04\n",
            "  2.73544085e-03  2.65763607e-04 -5.79310348e-03 -2.21614420e-04\n",
            "  3.13446228e-03 -1.23969431e-03  3.25390627e-03 -1.06587075e-04\n",
            "  3.93592147e-03  8.79614439e-04  4.25744336e-03 -1.01842918e-03\n",
            "  9.04095650e-04  1.03478087e-04  1.20514247e-03 -3.23425676e-03\n",
            " -3.97296483e-03 -8.87365080e-04  1.80697720e-03 -6.99928962e-03\n",
            "  7.22442335e-03 -1.17927336e-03  1.76682090e-03  6.77613169e-03\n",
            " -2.14114063e-03  6.61642430e-03  1.21497619e-03 -2.90035270e-03\n",
            "  1.49896333e-03 -3.90145765e-03 -2.54906248e-03  1.23761885e-03\n",
            " -2.04229332e-03 -3.07604484e-03  2.15400965e-03 -1.16786733e-03\n",
            " -8.49742442e-04 -4.97345021e-03  4.45992831e-04  6.83927152e-04\n",
            "  6.75586984e-03  9.49405730e-05  6.86252955e-04  6.33082018e-05\n",
            " -1.60222093e-03 -6.96582720e-04  2.03142525e-03  2.07272042e-05\n",
            "  1.11112930e-03 -1.28784601e-03 -2.81527825e-03 -1.59515406e-03\n",
            " -2.25661276e-03  3.04877083e-03 -2.99621816e-03  2.91443523e-03\n",
            "  1.31750060e-03 -3.89727153e-04  3.62625881e-03  2.23224703e-03\n",
            " -6.90181449e-04  1.65285589e-03  4.46179882e-03  3.91941471e-03\n",
            " -3.83328740e-03  5.95073448e-03  1.61694211e-03  2.40592449e-03\n",
            "  2.98796245e-03  3.43278894e-04  1.08498672e-03  1.70619076e-03\n",
            "  4.21284552e-04  3.44943954e-04  5.85334550e-04  1.39607314e-03\n",
            "  4.53192182e-03 -5.67804417e-03 -5.45544876e-03  3.85623891e-04\n",
            "  2.19670916e-03  5.41929156e-04 -3.72495991e-03 -4.39097965e-03\n",
            "  5.15923416e-03  5.49455965e-03 -8.72251578e-04  2.57236790e-03\n",
            "  1.55994890e-03 -4.60150326e-03  3.48051824e-03 -1.47750907e-04\n",
            "  2.72719935e-03  1.56729959e-03  4.93073463e-03 -7.33330706e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y5aUDv8dWmy-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}